{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:06:34.289628Z","iopub.execute_input":"2023-06-30T11:06:34.290041Z","iopub.status.idle":"2023-06-30T11:06:47.886857Z","shell.execute_reply.started":"2023-06-30T11:06:34.290015Z","shell.execute_reply":"2023-06-30T11:06:47.885661Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.6.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile my_dataset.py\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass MyDataSet(Dataset):\n    \"\"\"自定义数据集\"\"\"\n\n    def __init__(self, images_path: list, images_class: list, transform=None):\n        self.images_path = images_path\n        self.images_class = images_class\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_path)\n\n    def __getitem__(self, item):\n        img = Image.open(self.images_path[item]).convert('RGB')\n        # RGB为彩色图片，L为灰度图片\n        \n        label = self.images_class[item]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, label\n\n    @staticmethod\n    def collate_fn(batch):\n        # 官方实现的default_collate可以参考\n        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n        images, labels = tuple(zip(*batch))\n\n        images = torch.stack(images, dim=0)\n        labels = torch.as_tensor(labels)\n        return images, labels","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-30T11:07:25.548445Z","iopub.execute_input":"2023-06-30T11:07:25.548857Z","iopub.status.idle":"2023-06-30T11:07:25.557024Z","shell.execute_reply.started":"2023-06-30T11:07:25.548824Z","shell.execute_reply":"2023-06-30T11:07:25.555636Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing my_dataset.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"![CSwin](https://github.com/microsoft/Cream/blob/main/TinyViT/.figure/framework.png?raw=true)","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py\nimport os\nimport sys\nimport json\nimport pickle\nimport random\n\nimport torch\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\n\ndef read_split_data(root: str, val_rate: float = 0.2):\n    random.seed(0)  # 保证随机结果可复现\n    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root)\n\n    # 遍历文件夹，一个文件夹对应一个类别\n    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]\n    # 排序，保证各平台顺序一致\n    flower_class.sort()\n    # 生成类别名称以及对应的数字索引\n    class_indices = dict((k, v) for v, k in enumerate(flower_class))\n    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)\n    with open('class_indices.json', 'w') as json_file:\n        json_file.write(json_str)\n\n    train_images_path = []  # 存储训练集的所有图片路径\n    train_images_label = []  # 存储训练集图片对应索引信息\n    val_images_path = []  # 存储验证集的所有图片路径\n    val_images_label = []  # 存储验证集图片对应索引信息\n    every_class_num = []  # 存储每个类别的样本总数\n    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  # 支持的文件后缀类型\n    # 遍历每个文件夹下的文件\n    for cla in flower_class:\n        cla_path = os.path.join(root, cla)\n        # 遍历获取supported支持的所有文件路径\n        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)\n                  if os.path.splitext(i)[-1] in supported]\n        # 排序，保证各平台顺序一致\n        images.sort()\n        # 获取该类别对应的索引\n        image_class = class_indices[cla]\n        # 记录该类别的样本数量\n        every_class_num.append(len(images))\n        # 按比例随机采样验证样本\n        val_path = random.sample(images, k=int(len(images) * val_rate))\n\n        for img_path in images:\n            if img_path in val_path:  # 如果该路径在采样的验证集样本中则存入验证集\n                val_images_path.append(img_path)\n                val_images_label.append(image_class)\n            else:  # 否则存入训练集\n                train_images_path.append(img_path)\n                train_images_label.append(image_class)\n\n    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n    print(\"{} images for training.\".format(len(train_images_path)))\n    print(\"{} images for validation.\".format(len(val_images_path)))\n    assert len(train_images_path) > 0, \"number of training images must greater than 0.\"\n    assert len(val_images_path) > 0, \"number of validation images must greater than 0.\"\n\n    plot_image = False\n    if plot_image:\n        # 绘制每种类别个数柱状图\n        plt.bar(range(len(flower_class)), every_class_num, align='center')\n        # 将横坐标0,1,2,3,4替换为相应的类别名称\n        plt.xticks(range(len(flower_class)), flower_class)\n        # 在柱状图上添加数值标签\n        for i, v in enumerate(every_class_num):\n            plt.text(x=i, y=v + 5, s=str(v), ha='center')\n        # 设置x坐标\n        plt.xlabel('image class')\n        # 设置y坐标\n        plt.ylabel('number of images')\n        # 设置柱状图的标题\n        plt.title('flower class distribution')\n        plt.show()\n\n    return train_images_path, train_images_label, val_images_path, val_images_label\n\n\ndef plot_data_loader_image(data_loader):\n    batch_size = data_loader.batch_size\n    plot_num = min(batch_size, 4)\n\n    json_path = './class_indices.json'\n    assert os.path.exists(json_path), json_path + \" does not exist.\"\n    json_file = open(json_path, 'r')\n    class_indices = json.load(json_file)\n\n    for data in data_loader:\n        images, labels = data\n        for i in range(plot_num):\n            # [C, H, W] -> [H, W, C]\n            img = images[i].numpy().transpose(1, 2, 0)\n            # 反Normalize操作\n            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255\n            label = labels[i].item()\n            plt.subplot(1, plot_num, i+1)\n            plt.xlabel(class_indices[str(label)])\n            plt.xticks([])  # 去掉x轴的刻度\n            plt.yticks([])  # 去掉y轴的刻度\n            plt.imshow(img.astype('uint8'))\n        plt.show()\n\n\ndef write_pickle(list_info: list, file_name: str):\n    with open(file_name, 'wb') as f:\n        pickle.dump(list_info, f)\n\n\ndef read_pickle(file_name: str) -> list:\n    with open(file_name, 'rb') as f:\n        info_list = pickle.load(f)\n        return info_list\n\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    loss_function = torch.nn.CrossEntropyLoss()\n    accu_loss = torch.zeros(1).to(device)  # 累计损失\n    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n    optimizer.zero_grad()\n\n    sample_num = 0\n    data_loader = tqdm(data_loader, file=sys.stdout)\n    for step, data in enumerate(data_loader):\n        images, labels = data\n        sample_num += images.shape[0]\n\n        pred = model(images.to(device))\n        pred_classes = torch.max(pred, dim=1)[1]\n        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n        labels = labels.to(device)\n        loss = loss_function(pred, labels)\n        loss.backward()\n        accu_loss += loss.detach()\n\n        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n                                                                               accu_loss.item() / (step + 1),\n                                                                               accu_num.item() / sample_num)\n\n        if not torch.isfinite(loss):\n            print('WARNING: non-finite loss, ending training ', loss)\n            sys.exit(1)\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return accu_loss.item() / (step + 1), accu_num.item() / sample_num\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device, epoch):\n    loss_function = torch.nn.CrossEntropyLoss()\n\n    model.eval()\n\n    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n    accu_loss = torch.zeros(1).to(device)  # 累计损失\n\n    sample_num = 0\n    data_loader = tqdm(data_loader, file=sys.stdout)\n    for step, data in enumerate(data_loader):\n        images, labels = data\n        sample_num += images.shape[0]\n\n        pred = model(images.to(device))\n        pred_classes = torch.max(pred, dim=1)[1]\n        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n\n        loss = loss_function(pred, labels.to(device))\n        accu_loss += loss\n\n        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n                                                                               accu_loss.item() / (step + 1),\n                                                                               accu_num.item() / sample_num)\n\n    return accu_loss.item() / (step + 1), accu_num.item() / sample_num","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:07:29.844243Z","iopub.execute_input":"2023-06-30T11:07:29.844631Z","iopub.status.idle":"2023-06-30T11:07:29.855945Z","shell.execute_reply.started":"2023-06-30T11:07:29.844602Z","shell.execute_reply":"2023-06-30T11:07:29.854910Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile tinyvit.py\n# ------------------------------------------\n# CSWin Transformer\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n# written By Xiaoyi Dong\n# ------------------------------------------\n\n\n# --------------------------------------------------------\n# TinyViT Model Architecture\n# Copyright (c) 2022 Microsoft\n# Adapted from LeViT and Swin Transformer\n#   LeViT: (https://github.com/facebookresearch/levit)\n#   Swin: (https://github.com/microsoft/swin-transformer)\n# Build the TinyViT Model\n# --------------------------------------------------------\n\nimport itertools\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport timm\nfrom timm.models.layers import DropPath as TimmDropPath,\\\n    to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\ntry:\n    # timm.__version__ >= \"0.6\"\n    from timm.models._builder import build_model_with_cfg\nexcept (ImportError, ModuleNotFoundError):\n    # timm.__version__ < \"0.6\"\n    from timm.models.helpers import build_model_with_cfg\n\n\nclass Conv2d_BN(torch.nn.Sequential):\n    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n                 groups=1, bn_weight_init=1):\n        super().__init__()\n        self.add_module('c', torch.nn.Conv2d(\n            a, b, ks, stride, pad, dilation, groups, bias=False))\n        bn = torch.nn.BatchNorm2d(b)\n        torch.nn.init.constant_(bn.weight, bn_weight_init)\n        torch.nn.init.constant_(bn.bias, 0)\n        self.add_module('bn', bn)\n\n    @torch.no_grad()\n    def fuse(self):\n        c, bn = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps)**0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / \\\n            (bn.running_var + bn.eps)**0.5\n        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n\nclass DropPath(TimmDropPath):\n    def __init__(self, drop_prob=None):\n        super().__init__(drop_prob=drop_prob)\n        self.drop_prob = drop_prob\n\n    def __repr__(self):\n        msg = super().__repr__()\n        msg += f'(drop_prob={self.drop_prob})'\n        return msg\n\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, in_chans, embed_dim, resolution, activation):\n        super().__init__()\n        img_size: Tuple[int, int] = to_2tuple(resolution)\n        self.patches_resolution = (img_size[0] // 4, img_size[1] // 4)\n        self.num_patches = self.patches_resolution[0] * \\\n            self.patches_resolution[1]\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        n = embed_dim\n        self.seq = nn.Sequential(\n            Conv2d_BN(in_chans, n // 2, 3, 2, 1),\n            activation(),\n            Conv2d_BN(n // 2, n, 3, 2, 1),\n        )\n\n    def forward(self, x):\n        return self.seq(x)\n\n\nclass MBConv(nn.Module):\n    def __init__(self, in_chans, out_chans, expand_ratio,\n                 activation, drop_path):\n        super().__init__()\n        self.in_chans = in_chans\n        self.hidden_chans = int(in_chans * expand_ratio)\n        self.out_chans = out_chans\n\n        self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\n        self.act1 = activation()\n\n        self.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans,\n                               ks=3, stride=1, pad=1, groups=self.hidden_chans)\n        self.act2 = activation()\n\n        self.conv3 = Conv2d_BN(\n            self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)\n        self.act3 = activation()\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n\n        x = self.conv1(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.act2(x)\n\n        x = self.conv3(x)\n\n        x = self.drop_path(x)\n\n        x += shortcut\n        x = self.act3(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    def __init__(self, input_resolution, dim, out_dim, activation):\n        super().__init__()\n\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.out_dim = out_dim\n        self.act = activation()\n        self.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)\n        self.conv2 = Conv2d_BN(out_dim, out_dim, 3, 2, 1, groups=out_dim)\n        self.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)\n\n    def forward(self, x):\n        if x.ndim == 3:\n            H, W = self.input_resolution\n            B = len(x)\n            # (B, C, H, W)\n            x = x.view(B, H, W, -1).permute(0, 3, 1, 2)\n\n        x = self.conv1(x)\n        x = self.act(x)\n        x = self.conv2(x)\n        x = self.act(x)\n        x = self.conv3(x)\n\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, dim, input_resolution, depth,\n                 activation,\n                 drop_path=0., downsample=None, use_checkpoint=False,\n                 out_dim=None,\n                 conv_expand_ratio=4.,\n                 ):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            MBConv(dim, dim, conv_expand_ratio, activation,\n                   drop_path[i] if isinstance(drop_path, list) else drop_path,\n                   )\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None,\n                 out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.norm = nn.LayerNorm(in_features)\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, dim, key_dim, num_heads=8,\n                 attn_ratio=4,\n                 resolution=(14, 14),\n                 ):\n        super().__init__()\n        # (h, w)\n        assert isinstance(resolution, tuple) and len(resolution) == 2\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n        self.nh_kd = nh_kd = key_dim * num_heads\n        self.d = int(attn_ratio * key_dim)\n        self.dh = int(attn_ratio * key_dim) * num_heads\n        self.attn_ratio = attn_ratio\n        h = self.dh + nh_kd * 2\n\n        self.norm = nn.LayerNorm(dim)\n        self.qkv = nn.Linear(dim, h)\n        self.proj = nn.Linear(self.dh, dim)\n\n        points = list(itertools.product(\n            range(resolution[0]), range(resolution[1])))\n        N = len(points)\n        attention_offsets = {}\n        idxs = []\n        for p1 in points:\n            for p2 in points:\n                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n                if offset not in attention_offsets:\n                    attention_offsets[offset] = len(attention_offsets)\n                idxs.append(attention_offsets[offset])\n        self.attention_biases = torch.nn.Parameter(\n            torch.zeros(num_heads, len(attention_offsets)))\n        self.register_buffer('attention_bias_idxs',\n                             torch.LongTensor(idxs).view(N, N),\n                             persistent=False)\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and hasattr(self, 'ab'):\n            del self.ab\n        else:\n            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n\n    def forward(self, x):  # x (B,N,C)\n        B, N, _ = x.shape\n\n        # Normalization\n        x = self.norm(x)\n\n        qkv = self.qkv(x)\n        # (B, N, num_heads, d)\n        q, k, v = qkv.view(B, N, self.num_heads, -\n                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n        # (B, num_heads, N, d)\n        q = q.permute(0, 2, 1, 3)\n        k = k.permute(0, 2, 1, 3)\n        v = v.permute(0, 2, 1, 3)\n\n        attn = (\n            (q @ k.transpose(-2, -1)) * self.scale\n            +\n            (self.attention_biases[:, self.attention_bias_idxs]\n             if self.training else self.ab)\n        )\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n        x = self.proj(x)\n        return x\n\n\nclass TinyViTBlock(nn.Module):\n    r\"\"\" TinyViT Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int, int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        local_conv_size (int): the kernel size of the convolution between\n                               Attention and MLP. Default: 3\n        activation: the activation function. Default: nn.GELU\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 local_conv_size=3,\n                 activation=nn.GELU,\n                 ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        assert window_size > 0, 'window_size must be greater than 0'\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n\n        assert dim % num_heads == 0, 'dim must be divisible by num_heads'\n        head_dim = dim // num_heads\n\n        window_resolution = (window_size, window_size)\n        self.attn = Attention(dim, head_dim, num_heads,\n                              attn_ratio=1, resolution=window_resolution)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        mlp_activation = activation\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=mlp_activation, drop=drop)\n\n        pad = local_conv_size // 2\n        self.local_conv = Conv2d_BN(\n            dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        res_x = x\n        if H == self.window_size and W == self.window_size:\n            x = self.attn(x)\n        else:\n            x = x.view(B, H, W, C)\n            pad_b = (self.window_size - H %\n                     self.window_size) % self.window_size\n            pad_r = (self.window_size - W %\n                     self.window_size) % self.window_size\n            padding = pad_b > 0 or pad_r > 0\n\n            if padding:\n                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n\n            pH, pW = H + pad_b, W + pad_r\n            nH = pH // self.window_size\n            nW = pW // self.window_size\n            # window partition\n            x = x.view(B, nH, self.window_size, nW, self.window_size, C).transpose(2, 3).reshape(\n                B * nH * nW, self.window_size * self.window_size, C\n            )\n            x = self.attn(x)\n            # window reverse\n            x = x.view(B, nH, nW, self.window_size, self.window_size,\n                       C).transpose(2, 3).reshape(B, pH, pW, C)\n\n            if padding:\n                x = x[:, :H, :W].contiguous()\n\n            x = x.view(B, L, C)\n\n        x = res_x + self.drop_path(x)\n\n        x = x.transpose(1, 2).reshape(B, C, H, W)\n        x = self.local_conv(x)\n        x = x.view(B, C, L).transpose(1, 2)\n\n        x = x + self.drop_path(self.mlp(x))\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, mlp_ratio={self.mlp_ratio}\"\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic TinyViT layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        local_conv_size: the kernel size of the depthwise convolution between attention and MLP. Default: 3\n        activation: the activation function. Default: nn.GELU\n        out_dim: the output dimension of the layer. Default: dim\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., drop=0.,\n                 drop_path=0., downsample=None, use_checkpoint=False,\n                 local_conv_size=3,\n                 activation=nn.GELU,\n                 out_dim=None,\n                 ):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            TinyViTBlock(dim=dim, input_resolution=input_resolution,\n                         num_heads=num_heads, window_size=window_size,\n                         mlp_ratio=mlp_ratio,\n                         drop=drop,\n                         drop_path=drop_path[i] if isinstance(\n                             drop_path, list) else drop_path,\n                         local_conv_size=local_conv_size,\n                         activation=activation,\n                         )\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n\nclass TinyViT(nn.Module):\n    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\n                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n                 num_heads=[3, 6, 12, 24],\n                 window_sizes=[7, 7, 14, 7],\n                 mlp_ratio=4.,\n                 drop_rate=0.,\n                 drop_path_rate=0.1,\n                 use_checkpoint=False,\n                 mbconv_expand_ratio=4.0,\n                 local_conv_size=3,\n                 layer_lr_decay=1.0,\n                 ):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_layers = len(depths)\n        self.mlp_ratio = mlp_ratio\n\n        activation = nn.GELU\n\n        self.patch_embed = PatchEmbed(in_chans=in_chans,\n                                      embed_dim=embed_dims[0],\n                                      resolution=img_size,\n                                      activation=activation)\n\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate,\n                                                sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            kwargs = dict(dim=embed_dims[i_layer],\n                          input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                            patches_resolution[1] // (2 ** i_layer)),\n                          depth=depths[i_layer],\n                          drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                          downsample=PatchMerging if (\n                              i_layer < self.num_layers - 1) else None,\n                          use_checkpoint=use_checkpoint,\n                          out_dim=embed_dims[min(\n                              i_layer + 1, len(embed_dims) - 1)],\n                          activation=activation,\n                          )\n            if i_layer == 0:\n                layer = ConvLayer(\n                    conv_expand_ratio=mbconv_expand_ratio,\n                    **kwargs,\n                )\n            else:\n                layer = BasicLayer(\n                    num_heads=num_heads[i_layer],\n                    window_size=window_sizes[i_layer],\n                    mlp_ratio=self.mlp_ratio,\n                    drop=drop_rate,\n                    local_conv_size=local_conv_size,\n                    **kwargs)\n            self.layers.append(layer)\n\n        # Classifier head\n        self.norm_head = nn.LayerNorm(embed_dims[-1])\n        self.head = nn.Linear(\n            embed_dims[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n\n        # init weights\n        self.apply(self._init_weights)\n        self.set_layer_lr_decay(layer_lr_decay)\n\n    def set_layer_lr_decay(self, layer_lr_decay):\n        decay_rate = layer_lr_decay\n\n        # layers -> blocks (depth)\n        depth = sum(self.depths)\n        lr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]\n\n        def _set_lr_scale(m, scale):\n            for p in m.parameters():\n                p.lr_scale = scale\n\n        self.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))\n        i = 0\n        for layer in self.layers:\n            for block in layer.blocks:\n                block.apply(lambda x: _set_lr_scale(x, lr_scales[i]))\n                i += 1\n            if layer.downsample is not None:\n                layer.downsample.apply(\n                    lambda x: _set_lr_scale(x, lr_scales[i - 1]))\n        assert i == depth\n        for m in [self.norm_head, self.head]:\n            m.apply(lambda x: _set_lr_scale(x, lr_scales[-1]))\n\n        for k, p in self.named_parameters():\n            p.param_name = k\n\n        def _check_lr_scale(m):\n            for p in m.parameters():\n                assert hasattr(p, 'lr_scale'), p.param_name\n\n        self.apply(_check_lr_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'attention_biases'}\n\n    def forward_features(self, x):\n        # x: (N, C, H, W)\n        x = self.patch_embed(x)\n\n        x = self.layers[0](x)\n        start_i = 1\n\n        for i in range(start_i, len(self.layers)):\n            layer = self.layers[i]\n            x = layer(x)\n\n        x = x.mean(1)\n\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.norm_head(x)\n        x = self.head(x)\n        return x\n\n\n_checkpoint_url_format = \\\n    'https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/{}.pth'\n\n\ndef _create_tiny_vit(variant, pretrained=False, **kwargs):\n    # pretrained_type: 22kto1k_distill, 1k, 22k_distill\n    pretrained_type = kwargs.pop('pretrained_type', '22kto1k_distill')\n    assert pretrained_type in ['22kto1k_distill', '1k', '22k_distill'], \\\n        'pretrained_type should be one of 22kto1k_distill, 1k, 22k_distill'\n\n    img_size = kwargs.get('img_size', 224)\n    if img_size != 224:\n        pretrained_type = pretrained_type.replace('_', f'_{img_size}_')\n\n    num_classes_pretrained = 21841 if \\\n        pretrained_type  == '22k_distill' else 1000\n\n    variant_without_img_size = '_'.join(variant.split('_')[:-1])\n    cfg = dict(\n        url=_checkpoint_url_format.format(\n            f'{variant_without_img_size}_{pretrained_type}'),\n        num_classes=num_classes_pretrained,\n        classifier='head',\n    )\n\n    def _pretrained_filter_fn(state_dict):\n        state_dict = state_dict['model']\n        # filter out attention_bias_idxs\n        state_dict = {k: v for k, v in state_dict.items() if \\\n            not k.endswith('attention_bias_idxs')}\n        return state_dict\n\n    if timm.__version__ >= \"0.6\":\n        return build_model_with_cfg(\n            TinyViT, variant, pretrained,\n            pretrained_cfg=cfg,\n            pretrained_filter_fn=_pretrained_filter_fn,\n            **kwargs)\n    else:\n        return build_model_with_cfg(\n            TinyViT, variant, pretrained,\n            default_cfg=cfg,\n            pretrained_filter_fn=_pretrained_filter_fn,\n            **kwargs)\n\n\n@register_model\ndef tiny_vit_5m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[64, 128, 160, 320],\n        depths=[2, 2, 6, 2],\n        num_heads=[2, 4, 5, 10],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.0,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_5m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_11m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[64, 128, 256, 448],\n        depths=[2, 2, 6, 2],\n        num_heads=[2, 4, 8, 14],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_11m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.2,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_384(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        img_size=384,\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[12, 12, 24, 12],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_384', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_512(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        img_size=512,\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[16, 16, 32, 16],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_512', pretrained, **model_kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:07:32.510527Z","iopub.execute_input":"2023-06-30T11:07:32.510887Z","iopub.status.idle":"2023-06-30T11:07:32.535787Z","shell.execute_reply.started":"2023-06-30T11:07:32.510855Z","shell.execute_reply":"2023-06-30T11:07:32.534713Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing tinyvit.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train.py\nimport os\nimport argparse\n\nimport torch\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import transforms\n\nfrom my_dataset import MyDataSet\n#from model import swin_tiny_patch4_window7_224 as create_model\nfrom utils import read_split_data, train_one_epoch, evaluate\nimport timm\nfrom tinyvit import TinyViT,_create_tiny_vit\n\n\ndef main(args):\n    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n\n    if os.path.exists(\"./weights\") is False:\n        os.makedirs(\"./weights\")\n\n    tb_writer = SummaryWriter()\n\n    train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)\n\n    img_size = 224\n    data_transform = {\n        \"train\": transforms.Compose([transforms.RandomResizedCrop(img_size),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n        \"val\": transforms.Compose([transforms.Resize(int(img_size * 1.143)),\n                                   transforms.CenterCrop(img_size),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n\n    # 实例化训练数据集\n    train_dataset = MyDataSet(images_path=train_images_path,\n                              images_class=train_images_label,\n                              transform=data_transform[\"train\"])\n\n    # 实例化验证数据集\n    val_dataset = MyDataSet(images_path=val_images_path,\n                            images_class=val_images_label,\n                            transform=data_transform[\"val\"])\n\n    batch_size = args.batch_size\n    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n    print('Using {} dataloader workers every process'.format(nw))\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               pin_memory=True,\n                                               num_workers=nw,\n                                               collate_fn=train_dataset.collate_fn)\n\n    val_loader = torch.utils.data.DataLoader(val_dataset,\n                                             batch_size=batch_size,\n                                             shuffle=False,\n                                             pin_memory=True,\n                                             num_workers=nw,\n                                             collate_fn=val_dataset.collate_fn)\n\n    #model = CSWinTransformer(patch_size=4, num_classes=args.num_classes, embed_dim=144, depth=[2,4,32,2],split_size=[1,2,7,7], num_heads=[6,12,24,24], mlp_ratio=4.).to(device)\n    model_kwargs = dict(\n        embed_dims=[96, 192, 384, 576],\n        num_classes=args.num_classes,\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.2,\n    )\n    \n    model = _create_tiny_vit('tiny_vit_21m_224', pretrained=True, **model_kwargs).cuda()\n    #model = timm.create_model('deit_small_patch16_224', pretrained=True,num_classes=args.num_classes).to(device)\n    #model = timm.create_model('convnext_small_in22k',pretrained=True,num_classes = args.num_classes).to(device)\n    if args.weights != \"\":\n        assert os.path.exists(args.weights), \"weights file: '{}' not exist.\".format(args.weights)\n        '''weights_dict = torch.load(args.weights, map_location=device)\n        # 删除有关分类类别的权重\n        pre_dict = {k: v for k, v in weights_dict.items() if \"classifier\" not in k}\n# 另一种方法会直接两种权重对比，直接两种方法对比，减少问题的存在\n        pre_dict = {k: v for k, v in weights_dict.items() \n            if net_weights[k].numel() == v.numel()}\n\n        model.load_state_dict(pre_dict, strict=False)'''\n        pretrained_dict = torch.load(args.weights, map_location=device)\n        model_dict = model.state_dict()\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n\n        #print(model.load_state_dict(weights_dict, strict=False))\n\n    if args.freeze_layers:\n        for name, para in model.named_parameters():\n            # 除head外，其他权重全部冻结\n            if \"head\" not in name:\n                para.requires_grad_(False)\n            else:\n                print(\"training {}\".format(name))\n\n    pg = [p for p in model.parameters() if p.requires_grad]\n    optimizer = optim.AdamW(pg, lr=args.lr, weight_decay=5E-3)\n\n    for epoch in range(args.epochs):\n        # train\n        train_loss, train_acc = train_one_epoch(model=model,\n                                                optimizer=optimizer,\n                                                data_loader=train_loader,\n                                                device=device,\n                                                epoch=epoch)\n\n        # validate\n        val_loss, val_acc = evaluate(model=model,\n                                     data_loader=val_loader,\n                                     device=device,\n                                     epoch=epoch)\n\n        tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n        tb_writer.add_scalar(tags[0], train_loss, epoch)\n        tb_writer.add_scalar(tags[1], train_acc, epoch)\n        tb_writer.add_scalar(tags[2], val_loss, epoch)\n        tb_writer.add_scalar(tags[3], val_acc, epoch)\n        tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n\n        torch.save(model.state_dict(), \"last.pth\".format(epoch))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num_classes', type=int, default=5)\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=8)\n    parser.add_argument('--lr', type=float, default=0.0001)\n\n    # 数据集所在根目录\n    # https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n    parser.add_argument('--data-path', type=str,\n                        default=\"/data/flower_photos\")\n\n    # 预训练权重路径，如果不想载入就设置为空字符\n    parser.add_argument('--weights', type=str, default='',\n                        help='initial weights path')\n    # 是否冻结权重\n    parser.add_argument('--freeze-layers', type=bool, default=False)\n    parser.add_argument('--device', default='cuda:0', help='device id (i.e. 0 or 0,1 or cpu)')\n\n    opt = parser.parse_args()\n\n    main(opt)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:10:52.243837Z","iopub.execute_input":"2023-06-30T11:10:52.244222Z","iopub.status.idle":"2023-06-30T11:10:52.256075Z","shell.execute_reply.started":"2023-06-30T11:10:52.244190Z","shell.execute_reply":"2023-06-30T11:10:52.255100Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python train.py --data-path /kaggle/input/5-flower-types-classification-dataset/flower_images --num_classes 5 --epochs 20 --batch-size 80 ","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:11:41.360364Z","iopub.execute_input":"2023-06-30T11:11:41.360761Z","iopub.status.idle":"2023-06-30T11:26:01.352760Z","shell.execute_reply.started":"2023-06-30T11:11:41.360729Z","shell.execute_reply":"2023-06-30T11:26:01.351477Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n4999 images were found in the dataset.\n4000 images for training.\n999 images for validation.\nUsing 2 dataloader workers every process\n[train epoch 0] loss: 1.240, acc: 0.707: 100%|██| 50/50 [00:36<00:00,  1.35it/s]\n[valid epoch 0] loss: 0.629, acc: 0.940: 100%|██| 13/13 [00:10<00:00,  1.27it/s]\n[train epoch 1] loss: 0.606, acc: 0.877: 100%|██| 50/50 [00:33<00:00,  1.49it/s]\n[valid epoch 1] loss: 0.230, acc: 0.955: 100%|██| 13/13 [00:07<00:00,  1.72it/s]\n[train epoch 2] loss: 0.353, acc: 0.908: 100%|██| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 2] loss: 0.155, acc: 0.964: 100%|██| 13/13 [00:07<00:00,  1.69it/s]\n[train epoch 3] loss: 0.281, acc: 0.920: 100%|██| 50/50 [00:34<00:00,  1.47it/s]\n[valid epoch 3] loss: 0.119, acc: 0.972: 100%|██| 13/13 [00:07<00:00,  1.72it/s]\n[train epoch 4] loss: 0.239, acc: 0.930: 100%|██| 50/50 [00:35<00:00,  1.42it/s]\n[valid epoch 4] loss: 0.101, acc: 0.978: 100%|██| 13/13 [00:07<00:00,  1.70it/s]\n[train epoch 5] loss: 0.201, acc: 0.944: 100%|██| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 5] loss: 0.083, acc: 0.977: 100%|██| 13/13 [00:07<00:00,  1.68it/s]\n[train epoch 6] loss: 0.172, acc: 0.948: 100%|██| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 6] loss: 0.080, acc: 0.980: 100%|██| 13/13 [00:07<00:00,  1.69it/s]\n[train epoch 7] loss: 0.162, acc: 0.947: 100%|██| 50/50 [00:34<00:00,  1.43it/s]\n[valid epoch 7] loss: 0.067, acc: 0.984: 100%|██| 13/13 [00:07<00:00,  1.72it/s]\n[train epoch 8] loss: 0.148, acc: 0.952: 100%|██| 50/50 [00:33<00:00,  1.49it/s]\n[valid epoch 8] loss: 0.067, acc: 0.982: 100%|██| 13/13 [00:07<00:00,  1.75it/s]\n[train epoch 9] loss: 0.140, acc: 0.956: 100%|██| 50/50 [00:33<00:00,  1.48it/s]\n[valid epoch 9] loss: 0.060, acc: 0.984: 100%|██| 13/13 [00:07<00:00,  1.67it/s]\n[train epoch 10] loss: 0.132, acc: 0.959: 100%|█| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 10] loss: 0.061, acc: 0.981: 100%|█| 13/13 [00:08<00:00,  1.57it/s]\n[train epoch 11] loss: 0.130, acc: 0.959: 100%|█| 50/50 [00:33<00:00,  1.48it/s]\n[valid epoch 11] loss: 0.061, acc: 0.983: 100%|█| 13/13 [00:07<00:00,  1.71it/s]\n[train epoch 12] loss: 0.121, acc: 0.960: 100%|█| 50/50 [00:34<00:00,  1.47it/s]\n[valid epoch 12] loss: 0.059, acc: 0.983: 100%|█| 13/13 [00:07<00:00,  1.67it/s]\n[train epoch 13] loss: 0.117, acc: 0.963: 100%|█| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 13] loss: 0.050, acc: 0.985: 100%|█| 13/13 [00:08<00:00,  1.59it/s]\n[train epoch 14] loss: 0.100, acc: 0.969: 100%|█| 50/50 [00:33<00:00,  1.48it/s]\n[valid epoch 14] loss: 0.045, acc: 0.986: 100%|█| 13/13 [00:07<00:00,  1.71it/s]\n[train epoch 15] loss: 0.100, acc: 0.969: 100%|█| 50/50 [00:33<00:00,  1.47it/s]\n[valid epoch 15] loss: 0.046, acc: 0.987: 100%|█| 13/13 [00:07<00:00,  1.67it/s]\n[train epoch 16] loss: 0.102, acc: 0.968: 100%|█| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 16] loss: 0.046, acc: 0.985: 100%|█| 13/13 [00:08<00:00,  1.54it/s]\n[train epoch 17] loss: 0.093, acc: 0.969: 100%|█| 50/50 [00:34<00:00,  1.46it/s]\n[valid epoch 17] loss: 0.045, acc: 0.985: 100%|█| 13/13 [00:07<00:00,  1.75it/s]\n[train epoch 18] loss: 0.084, acc: 0.973: 100%|█| 50/50 [00:33<00:00,  1.48it/s]\n[valid epoch 18] loss: 0.045, acc: 0.987: 100%|█| 13/13 [00:07<00:00,  1.74it/s]\n[train epoch 19] loss: 0.091, acc: 0.970: 100%|█| 50/50 [00:34<00:00,  1.47it/s]\n[valid epoch 19] loss: 0.048, acc: 0.983: 100%|█| 13/13 [00:08<00:00,  1.54it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}